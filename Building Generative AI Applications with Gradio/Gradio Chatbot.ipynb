{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ChatBot app with Gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import base64\n",
    "import requests, json\n",
    "import IPython.display\n",
    "from PIL import Image\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import gradio as gr\n",
    "import openai\n",
    "\n",
    "_ = load_dotenv(find_dotenv(filename=\"secrets.env\", raise_error_if_not_found=True))\n",
    "\n",
    "# Global variable\n",
    "ROOT_DIR = os.environ[\"ROOT_DIR\"]\n",
    "SYSTEM_PROMPT = \"\\\n",
    "\tYou are a helpful assistant and do your best to answer the user's questions.\\\n",
    "\tYou do not make up answers.\"\n",
    "\n",
    "# Load credential for API calls\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define and test the API calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "I'm doing well, thank you. How about you?\n"
     ]
    }
   ],
   "source": [
    "def APIcall(prompt:str, temperature = 0.7, max_tokens = 1024):\n",
    "\t\n",
    "\t# make the API call with the given parameter\n",
    "\tresponse = openai.completions.create(\n",
    "\t\tmodel=\"gpt-3.5-turbo-instruct\",\n",
    "\t\tprompt = prompt,\n",
    "\t\tmax_tokens = max_tokens,\n",
    "\t\ttemperature=temperature,\n",
    "\t)\n",
    "\n",
    "\t# return the completed text\n",
    "\toutput = response.choices[0].text\n",
    "\treturn output\n",
    "\n",
    "# test it\n",
    "print(APIcall(\"Hi, how are you doing?\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the ChatBot with Gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function: format the prompt to include history\n",
    "def formatPrompt(newMsg:str, chatHistory, instruction):\n",
    "\t\n",
    "\t# start with the system prompt\n",
    "\tprompt = f\"System: {instruction}\"\n",
    "\n",
    "\t# add the history\n",
    "\tfor turn in chatHistory:\n",
    "\t\tuserMsg, AssistantMsg = turn\n",
    "\t\tprompt+=f\"\\nUser: {userMsg}\\nAssistant: {AssistantMsg}\"\n",
    "\t\n",
    "\t# add the last message that needs to be answer\n",
    "\tprompt += f\"\\nUser: {newMsg}\\nAssistant: \"\n",
    "\n",
    "\t# return the formated prompt\n",
    "\treturn prompt\n",
    "\n",
    "# def the response function\n",
    "def response(prompt:str, chatHistory):\n",
    "\t\n",
    "\tchatHistory.append()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coucou c'est moi!\n"
     ]
    }
   ],
   "source": [
    "testText = \"coucou\"\n",
    "testText += \" c'est moi!\"\n",
    "print(testText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dataScienceVenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
